* Statistics

** What is the Karhunen–Loève theorem?

   Let $X_t$, $t\in[a,b]$ be a centered stochastic process with $\mathrm{E}[X_t]
   = 0$ for $t\in[a,b]$ Assume the process satifies a technical continuity
   condition. Then, we have
   \begin{equation}
     X_t = \sum_{k=1}^{\infty}
     Z_k e_k(t)
   \end{equation}
   where $Z_k$ are pairwise uncorrelated rando mvariables and $e_k$ are
   continuous real-valued functions on $[a,b]$ that are pariwise orthogonal in
   $L^2([a,b])$. If the process is Gaussian, then $Z_k$ are Gaussian and
   stochastically indepenedent.

   Given any orthonormal basis $e_k(t)$ of $L^2([a,b])$, we can approximate the
   stochastic process with
   \begin{equation}
     \hat{X}_t = \sum_{k=1}^{K} A_k\,e_k(t),\
     A_k =
       \int_a^b X_t\,e_k(t)\,\mathrm{d}t
     ,\, K\in\mathbb{N}
   \end{equation}
   The Karhunen–Loève expansion minimizes the total mean square error resulting
   of its truncation.

   Source: [[https://en.wikipedia.org/wiki/Kosambi%E2%80%93Karhunen%E2%80%93Lo%C3%A8ve_theorem][Kosambi–Karhunen–Loève theorem - Wikipedia]]

** What is the Karhunen–Loève decomposition of a Wiener process?

   Let $W_t$ be a Wiener process, i.e., a center standard Gaussian process with
   covariance function $K_{W}(t,s)=\operatorname {cov} (W_{t},W_{s})=\min(s,t)$.
   Then, the expansion consists of sinusoidal functions
   \begin{align}
     e_{k}(t)
     &={\sqrt{2}}\sin\left(\left(k-{\tfrac{1}{2}}\right)\pit\right)
     &\text{eigenfunctions}\\
     \lambda_{k}
     &=\frac{1}{(k-{\frac{1}{2}})^{2}\pi^{2}}
     &\text{eigenvalues}
   \end{align}

   Source: [[https://en.wikipedia.org/wiki/Kosambi%E2%80%93Karhunen%E2%80%93Lo%C3%A8ve_theorem#The_Wiener_process][Kosambi–Karhunen–Loève theorem - Wikipedia]]

** Principal Component Regression

   Let $\mathbf{T} = \mathbf{X} \mathbf{W}$ for principal component
   score matrix $\mathbf{T}$ and loading matrix $\mathbf{W}$. Set the
   models $Y = \mathbf{X} \mathbf{\beta} + \mathbf{\varepsilon}$ and
   $Y = \mathbf{T} \mathbf{\beta}_T + \mathbf{\varepsilon}$. Then,
   $\mathbf{X} \mathbf{\beta} = \mathbf{T} \mathbf{\beta}_T \iff
   \mathbf{\beta} = \mathbf{W} \mathbf{\beta}_T$.

** How to revert SVD?

   - [[https://stats.stackexchange.com/a/229093/31243][How to reverse PCA and reconstruct original variables from
     several principal components?]]
   - [[https://stats.stackexchange.com/a/134283/31243][Relationship between SVD and PCA. How to use SVD to perform PCA?]]

** How to find which variables are collinear?

   Look at the tail of the QR decomposition pivot vector (rickyrick at
   libera.chat)

   - [[https://stats.stackexchange.com/a/476216/31243][How to identify which variables are collinear in a singul...]]
   - [[https://en.wikipedia.org/wiki/QR_decomposition#Column_pivoting][QR decomposition - Column pivoting]]

** What classification model to use for observations in a map?

   - [[https://en.wikipedia.org/wiki/Conditional_random_field][Conditional random field]]: considers the context (neighborhood) of an
     observation when predicting a label
   - [[https://en.wikipedia.org/wiki/Markov_random_field][Markov random field]]: finite or infinite undirected graphical model that can
     do cyclic dependencies but can't do induced dependencies

** How to compute the Cressie and Hawkins (1980) robust estimator?

   Cressie and Hawkins (1980) found that the fourth-root of χ1 has a
   skewness of 0.08 and a kurtosis of 2.48 (compared with 0 and 3 for
   the Gaussian distribution). Estimates of location, such as the mean
   and the median, can then be applied to sqrt(X). Finally, these
   estimates can be raised to the 4th power and adjusted for
   bias. Consider the /square-root-differences cloud/ for
   visualization.

   - Source: [[https://www.doi.org/10.1002/9781119115151][10.1002/9781119115151]] eq. (2.2.8)

** How to transform a covariate taking non-negative values with zeroes?

   Consider a covariate (AKA independent variable, regressor, predictor) with
   true zeroes observed.
   - $log(x+1)$ which maps 0 to 0
   - $log(x+c)$ where $c$ is either estimated or set to be some small positive
     value
     - [[https://stats.stackexchange.com/a/1496/31243][Model sensitivity rule of thumb]]
   - inverse hyperbolic sine, which behaves like a log for large values
   - replace with two variables: a binary indicator for zero, and $log(x)$ for
     if $x$ is nonzero or zero otherwise
     (see Hosmer & Lemeshow's book)
     - probability plots of the positive part of the original variable are
       useful for identifying an appropriate re-expression
   - Box-Cox
   - Square root, or [[https://journals.sagepub.com/doi/pdf/10.1177/1536867X1101100112][cube root]], for something quick and dirty to get you going

   - Source
     - [[https://stats.stackexchange.com/questions/1444/how-should-i-transform-non-negative-data-including-zeros][How should I transform non-negative data including zeros?]]
     - [[https://robjhyndman.com/hyndsight/transformations/][Transforming data with zeros | Rob J Hyndman]]

** Priors

   - [[http://www.stats.org.uk/priors/noninformative/YangBerger1998.pdf][A Catalog of Noninformative Priors]]

** MCMC

   - [[http://users.stat.umn.edu/~geyer/mcmc/one.html][One long run in MCMC]]: If you can't get a good answer with one
     long run, then you can't get a good answer with many short runs
     either.
