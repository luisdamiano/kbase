* Statistics

** What is the Karhunen–Loève theorem?

   Let $X_t$, $t\in[a,b]$ be a centered stochastic process with $\mathrm{E}[X_t]
   = 0$ for $t\in[a,b]$ Assume the process satifies a technical continuity
   condition. Then, we have
   \begin{equation}
     X_t = \sum_{k=1}^{\infty}
     Z_k e_k(t)
   \end{equation}
   where $Z_k$ are pairwise uncorrelated rando mvariables and $e_k$ are
   continuous real-valued functions on $[a,b]$ that are pariwise orthogonal in
   $L^2([a,b])$. If the process is Gaussian, then $Z_k$ are Gaussian and
   stochastically indepenedent.

   Given any orthonormal basis $e_k(t)$ of $L^2([a,b])$, we can approximate the
   stochastic process with
   \begin{equation}
     \hat{X}_t = \sum_{k=1}^{K} A_k\,e_k(t),\
     A_k =
       \int_a^b X_t\,e_k(t)\,\mathrm{d}t
     ,\, K\in\mathbb{N}
   \end{equation}
   The Karhunen–Loève expansion minimizes the total mean square error resulting
   of its truncation.

   Source: [[https://en.wikipedia.org/wiki/Kosambi%E2%80%93Karhunen%E2%80%93Lo%C3%A8ve_theorem][Kosambi–Karhunen–Loève theorem - Wikipedia]]

** What is the Karhunen–Loève decomposition of a Wiener process?

   Let $W_t$ be a Wiener process, i.e., a center standard Gaussian process with
   covariance function $K_{W}(t,s)=\operatorname {cov} (W_{t},W_{s})=\min(s,t)$.
   Then, the expansion consists of sinusoidal functions
   \begin{align}
     e_{k}(t)
     &={\sqrt{2}}\sin\left(\left(k-{\tfrac{1}{2}}\right)\pit\right)
     &\text{eigenfunctions}\\
     \lambda_{k}
     &=\frac{1}{(k-{\frac{1}{2}})^{2}\pi^{2}}
     &\text{eigenvalues}
   \end{align}

   Source: [[https://en.wikipedia.org/wiki/Kosambi%E2%80%93Karhunen%E2%80%93Lo%C3%A8ve_theorem#The_Wiener_process][Kosambi–Karhunen–Loève theorem - Wikipedia]]

** Principal Component Regression

   Let $\mathbf{T} = \mathbf{X} \mathbf{W}$ for principal component
   score matrix $\mathbf{T}$ and loading matrix $\mathbf{W}$. Set the
   models $Y = \mathbf{X} \mathbf{\beta} + \mathbf{\varepsilon}$ and
   $Y = \mathbf{T} \mathbf{\beta}_T + \mathbf{\varepsilon}$. Then,
   $\mathbf{X} \mathbf{\beta} = \mathbf{T} \mathbf{\beta}_T \iff
   \mathbf{\beta} = \mathbf{W} \mathbf{\beta}_T$.

** How to revert SVD?

   - [[https://stats.stackexchange.com/a/229093/31243][How to reverse PCA and reconstruct original variables from
     several principal components?]]
   - [[https://stats.stackexchange.com/a/134283/31243][Relationship between SVD and PCA. How to use SVD to perform PCA?]]

** How to find which variables are collinear?

   Look at the tail of the QR decomposition pivot vector (rickyrick at
   libera.chat)

   - [[https://stats.stackexchange.com/a/476216/31243][How to identify which variables are collinear in a singul...]]
   - [[https://en.wikipedia.org/wiki/QR_decomposition#Column_pivoting][QR decomposition - Column pivoting]]

** How to compute the Cressie and Hawkins (1980) robust estimator?

   Cressie and Hawkins (1980) found that the fourth-root of χ1 has a
   skewness of 0.08 and a kurtosis of 2.48 (compared with 0 and 3 for
   the Gaussian distribution). Estimates of location, such as the mean
   and the median, can then be applied to sqrt(X). Finally, these
   estimates can be raised to the 4th power and adjusted for
   bias. Consider the /square-root-differences cloud/ for
   visualization.

   - Source: [[https://www.doi.org/10.1002/9781119115151][10.1002/9781119115151]] eq. (2.2.8)

** Priors

   - [[http://www.stats.org.uk/priors/noninformative/YangBerger1998.pdf][A Catalog of Noninformative Priors]]

** MCMC

   - [[http://users.stat.umn.edu/~geyer/mcmc/one.html][One long run in MCMC]]: If you can't get a good answer with one
     long run, then you can't get a good answer with many short runs
     either.
