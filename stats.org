* Statistics

** Principal Component Regression

   Let $\mathbf{T} = \mathbf{X} \mathbf{W}$ for principal component
   score matrix $\mathbf{T}$ and loading matrix $\mathbf{W}$. Set the
   models $Y = \mathbf{X} \mathbf{\beta} + \mathbf{\varepsilon}$ and
   $Y = \mathbf{T} \mathbf{\beta}_T + \mathbf{\varepsilon}$. Then,
   $\mathbf{X} \mathbf{\beta} = \mathbf{T} \mathbf{\beta}_T \iff
   \mathbf{\beta} = \mathbf{W} \mathbf{\beta}_T$.

** How to revert SVD?

   - [[https://stats.stackexchange.com/a/229093/31243][How to reverse PCA and reconstruct original variables from
     several principal components?]]
   - [[https://stats.stackexchange.com/a/134283/31243][Relationship between SVD and PCA. How to use SVD to perform PCA?]]

** How to find which variables are collinear?

   Look at the tail of the QR decomposition pivot vector (rickyrick at
   libera.chat)

   - [[https://stats.stackexchange.com/a/476216/31243][How to identify which variables are collinear in a singul...]]
   - [[https://en.wikipedia.org/wiki/QR_decomposition#Column_pivoting][QR decomposition - Column pivoting]]

** How to compute the Cressie and Hawkins (1980) robust estimator?

   Cressie and Hawkins (1980) found that the fourth-root of Ï‡1 has a
   skewness of 0.08 and a kurtosis of 2.48 (compared with 0 and 3 for
   the Gaussian distribution). Estimates of location, such as the mean
   and the median, can then be applied to sqrt(X). Finally, these
   estimates can be raised to the 4th power and adjusted for
   bias. Consider the /square-root-differences cloud/ for
   visualization.

   - Source: [[https://www.doi.org/10.1002/9781119115151][10.1002/9781119115151]] eq. (2.2.8)

** Priors

   - [[http://www.stats.org.uk/priors/noninformative/YangBerger1998.pdf][A Catalog of Noninformative Priors]]

** MCMC

   - [[http://users.stat.umn.edu/~geyer/mcmc/one.html][One long run in MCMC]]: If you can't get a good answer with one
     long run, then you can't get a good answer with many short runs
     either.
